{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "## Annotation of raw data sets\n",
    "### Annotation Process\n",
    "Every student of the course helped annotate 500 tweets/Weibo sentences each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "## Implementation of simple Sentiment Analysis using only emission parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading File\n",
    "To easily iterate through a desired file, we strip the white space in the string and put separate words with their tags in a whole list.\n",
    "```\n",
    "def load_original_train():\n",
    "    with open(\"train\", encoding=\"utf-8\") as file:\n",
    "        train_list = file.readlines()\n",
    "        train_list = [x.strip() for x in train_list]\n",
    "        return train_list\n",
    "    \n",
    "train_list = load_original_train()\n",
    "```\n",
    "### Estimating emission parameters\n",
    "The method to estimate the emission parameters were as shown in the project description. First, we created an annotation dictionary to store counts of all possible state, {O, I-Positive, I-Negative, I-Neutral, B-Positive, B-Negative, B-Neutral}. Hence, we can call on count(y) anytime when estimating emission parameters without recounting them for every word.\n",
    " \n",
    "Then, to estimate emission, we created a function called estimate_emission which take in two variables, word and tag. In this function, the tag will call the annotation dictionary from the first part to yield us the relevant denominator named countBottom.\n",
    " \n",
    "For the numerator, a for loop through the loaded train_list above is employed. For each word with their tag, we further split them into a list with 2 elements, [word,tag]. This can only be done if the “word” is not an empty string “”, found between different tweets. Creating an if statement to calculate only when we have a word and tag. We loop through the file and increase numerator count, named countTop when we have the specified word and tag. After this is done, the function returns the fraction countTop/countBottom as the estimated_emission parameter.\n",
    "\n",
    "### Replacing words that appear less than k times with #UNK#\n",
    "To replace words that appear less than k times with #UNK#, the function modify_trainingset(k) was created.\n",
    "Firstly, we need to find a way to store the count of words so that we can know the words that have to be replaced. A dictionary called wordCountDict was created. We loop through the entire training set and counting the occurence of each word. \n",
    " \n",
    "Secondly, in order to iterate through the file just once and replace the words that needs to be replaced along the way, a list called wordToBeReplacedList was created. If the key in wordCountDict has a value less than k, it will be stored in wordToBeReplacedList.\n",
    " \n",
    "Thirdly, we iterate through train_list, and replace the word found in wordToBeReplacedList with #UNK#., then taking into account the original form of the file, the modified set of words and tags are written into a new file called modified_train\n",
    "\n",
    "### Implementing sentiment analysis system to produce tag\n",
    "In order to speed up the sentiment analysis algorithm, we created a giantEmissionDict to store all the possible emission parameters into it. This is done using the store_estimate_emission_fix function. The function is also done using the modified training set so we can count the emission of #UNK# as well. \n",
    "\n",
    "In sentiment analysis, we first check whether the word exists in the modified training set or not. If it does not, the word is treated as #UNK#. Then, we loop through all the possible tags and get it's emission with respect to that tag and storing them into scoreDict. The tag with the highest emission value is then picked. \n",
    "\n",
    "We then use the same sentiment_analysis function for all the words in the entire dev.in file and output the results to dev.p2.out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for Part 2\n",
    "#### CN\n",
    "```\n",
    "#Entity in gold data: 362\n",
    "#Entity in prediction: 3318\n",
    "\n",
    "#Correct Entity : 183\n",
    "Entity  precision: 0.0552\n",
    "Entity  recall: 0.5055\n",
    "Entity  F: 0.0995\n",
    "\n",
    "#Correct Sentiment : 57\n",
    "Sentiment  precision: 0.0172\n",
    "Sentiment  recall: 0.1575\n",
    "Sentiment  F: 0.0310\n",
    "```\n",
    "\n",
    "#### EN\n",
    "```\n",
    "#Entity in gold data: 226\n",
    "#Entity in prediction: 1201\n",
    "\n",
    "#Correct Entity : 165\n",
    "Entity  precision: 0.1374\n",
    "Entity  recall: 0.7301\n",
    "Entity  F: 0.2313\n",
    "\n",
    "#Correct Sentiment : 71\n",
    "Sentiment  precision: 0.0591\n",
    "Sentiment  recall: 0.3142\n",
    "Sentiment  F: 0.0995\n",
    "```\n",
    "\n",
    "#### FR \n",
    "```\n",
    "#Entity in gold data: 223\n",
    "#Entity in prediction: 1149\n",
    "\n",
    "#Correct Entity : 182\n",
    "Entity  precision: 0.1584\n",
    "Entity  recall: 0.8161\n",
    "Entity  F: 0.2653\n",
    "\n",
    "#Correct Sentiment : 68\n",
    "Sentiment  precision: 0.0592\n",
    "Sentiment  recall: 0.3049\n",
    "Sentiment  F: 0.0991\n",
    "```\n",
    "\n",
    "#### SG\n",
    "```\n",
    "#Entity in gold data: 1382\n",
    "#Entity in prediction: 6599\n",
    "\n",
    "#Correct Entity : 794\n",
    "Entity  precision: 0.1203\n",
    "Entity  recall: 0.5745\n",
    "Entity  F: 0.1990\n",
    "\n",
    "#Correct Sentiment : 315\n",
    "Sentiment  precision: 0.0477\n",
    "Sentiment  recall: 0.2279\n",
    "Sentiment  F: 0.0789\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "## Implementation of Viterbi\n",
    "### Estimating transition parameters using Maximum Likelihood Estimation\n",
    "We loop through the modified training set and we created a nested list to store tags of each sentence. We also append “START” and “STOP” to the list to allow easy counting later of the transition parameters. \n",
    "\n",
    "We then create a dictionary that contains the count of every possible transition in the modified training set. To calculate the transition parameter, we simply divide that count by the count of the start state, which is available to us from annotationDict that we have created earlier. The result is then stored into giantTransitionDict, with key being the transition. \n",
    "\n",
    "### Results for Part 3\n",
    "#### CN\n",
    "```\n",
    "#Entity in gold data: 362\n",
    "#Entity in prediction: 158\n",
    "\n",
    "#Correct Entity : 30\n",
    "Entity  precision: 0.1899\n",
    "Entity  recall: 0.0829\n",
    "Entity  F: 0.1154\n",
    "\n",
    "#Correct Sentiment : 22\n",
    "Sentiment  precision: 0.1392\n",
    "Sentiment  recall: 0.0608\n",
    "Sentiment  F: 0.0846\n",
    "```\n",
    "\n",
    "#### EN\n",
    "```\n",
    "#Entity in gold data: 226\n",
    "#Entity in prediction: 155\n",
    "\n",
    "#Correct Entity : 99\n",
    "Entity  precision: 0.6387\n",
    "Entity  recall: 0.4381\n",
    "Entity  F: 0.5197\n",
    "\n",
    "#Correct Sentiment : 62\n",
    "Sentiment  precision: 0.4000\n",
    "Sentiment  recall: 0.2743\n",
    "Sentiment  F: 0.3255\n",
    "```\n",
    "\n",
    "#### FR \n",
    "```\n",
    "#Entity in gold data: 223\n",
    "#Entity in prediction: 166\n",
    "\n",
    "#Correct Entity : 112\n",
    "Entity  precision: 0.6747\n",
    "Entity  recall: 0.5022\n",
    "Entity  F: 0.5758\n",
    "\n",
    "#Correct Sentiment : 72\n",
    "Sentiment  precision: 0.4337\n",
    "Sentiment  recall: 0.3229\n",
    "Sentiment  F: 0.3702\n",
    "```\n",
    "\n",
    "#### SG\n",
    "```\n",
    "#Entity in gold data: 1382\n",
    "#Entity in prediction: 723\n",
    "\n",
    "#Correct Entity : 135\n",
    "Entity  precision: 0.1867\n",
    "Entity  recall: 0.0977\n",
    "Entity  F: 0.1283\n",
    "\n",
    "#Correct Sentiment : 73\n",
    "Sentiment  precision: 0.1010\n",
    "Sentiment  recall: 0.0528\n",
    "Sentiment  F: 0.0694\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4\n",
    "## Implementation of Max Marginal algorithm\n",
    "### Results for Part 4\n",
    "#### CN\n",
    "```\n",
    "#Entity in gold data: 362\n",
    "#Entity in prediction: 191\n",
    "\n",
    "#Correct Entity : 68\n",
    "Entity  precision: 0.3560\n",
    "Entity  recall: 0.1878\n",
    "Entity  F: 0.2459\n",
    "\n",
    "#Correct Sentiment : 48\n",
    "Sentiment  precision: 0.2513\n",
    "Sentiment  recall: 0.1326\n",
    "Sentiment  F: 0.1736\n",
    "```\n",
    "\n",
    "#### EN\n",
    "```\n",
    "#Entity in gold data: 226\n",
    "#Entity in prediction: 172\n",
    "\n",
    "#Correct Entity : 104\n",
    "Entity  precision: 0.6047\n",
    "Entity  recall: 0.4602\n",
    "Entity  F: 0.5226\n",
    "\n",
    "#Correct Sentiment : 69\n",
    "Sentiment  precision: 0.4012\n",
    "Sentiment  recall: 0.3053\n",
    "Sentiment  F: 0.3467\n",
    "```\n",
    "\n",
    "#### FR \n",
    "```\n",
    "#Entity in gold data: 223\n",
    "#Entity in prediction: 173\n",
    "\n",
    "#Correct Entity : 113\n",
    "Entity  precision: 0.6532\n",
    "Entity  recall: 0.5067\n",
    "Entity  F: 0.5707\n",
    "\n",
    "#Correct Sentiment : 73\n",
    "Sentiment  precision: 0.4220\n",
    "Sentiment  recall: 0.3274\n",
    "Sentiment  F: 0.3687\n",
    "```\n",
    "\n",
    "#### SG\n",
    "```\n",
    "#Entity in gold data: 1382\n",
    "#Entity in prediction: 778\n",
    "\n",
    "#Correct Entity : 398\n",
    "Entity  precision: 0.5116\n",
    "Entity  recall: 0.2880\n",
    "Entity  F: 0.3685\n",
    "\n",
    "#Correct Sentiment : 259\n",
    "Sentiment  precision: 0.3329\n",
    "Sentiment  recall: 0.1874\n",
    "Sentiment  F: 0.2398\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5\n",
    "## Implementation of Viterbi with second order dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before implementing Second Order Viterbi\n",
    "We had to create a new transition parameter to takes into account the tags two states before. Thus, we had to create a new ```giantSecondTransitionDict```. It stores all the possible transition two states before and its value. Below is the pseudocode for the function that was used to create it:\n",
    "\n",
    "```\n",
    "def store_second_order_transition():\n",
    "    startStateCount = {}\n",
    "    transitionCount = {}\n",
    "    giantSecondTransitionDict = {}\n",
    "    for each_sentence_with_only_tags in entire_data_set_with_only_tags:\n",
    "        for i in range(len(each_sentence_with_only_tags)-1):\n",
    "            # count the occurence of the first two states and store it to startStateCount, e.g START O\n",
    "        for i in range(len(each_sentence_with_only_tags)-2):\n",
    "            # count the occurence of each transition, e.g, START O O, store it to transitionCount\n",
    "    for each_second_order_transition in transitionCount:\n",
    "        # calculate the transition parameter \n",
    "        # divide corresponding values of transitionCount by startStateCount\n",
    "        # store result into giantSecondTransitionDict\n",
    "    return giantSecondTransitionDict    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Second Order Viterbi\n",
    "The pseudocode below follows the implementation of the second order viterbi using the ```giantSecondTransitionDict``` created earlier:\n",
    "```\n",
    "def second_order_viterbi(sentence):\n",
    "    for each_word in sentence:\n",
    "        if each_word not in training set:\n",
    "            # treat it as #UNK#\n",
    "        \n",
    "        if at first word:\n",
    "            for each tag:\n",
    "                # piLayer = transition from START to tags\n",
    "                # store piLayer into piList\n",
    "        elif at second word:\n",
    "            for each tag:\n",
    "                # piLayer = transition from START and previous_tag to tags\n",
    "                # store piLayer into piList\n",
    "        else:\n",
    "            if word == \"\":\n",
    "                for previous_tags in previousLayer:\n",
    "                    # piLayer = transition from previous_tags to STOP\n",
    "                    # store piLayer into piList \n",
    "                # run backtrack_second_order(piList, sentence)\n",
    "                # in order to recover the path\n",
    "                \n",
    "            else:\n",
    "                for each tag:\n",
    "                    for previous_tags in previousLayer:\n",
    "                        # piLayer = transition from previous_tags to tags\n",
    "                        # store piLayer into piList \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtracking in Second Order Viterbi\n",
    "The following pseudocode allows the discovery of the optimal path by backtracking from the optimal score stored at each word:\n",
    "```\n",
    "def backtrack_second_order(piList, sentence):\n",
    "    path = []\n",
    "    # each key in piList's piLayer looks like this: \"O STOP\": 0.001, \"B-positive O\": 0.002 ....\n",
    "    for i in range(len(piList)-1):\n",
    "        if i == 0:\n",
    "            lastLayer = piList[len(piList) - (i+1)]\n",
    "            # pick the highest value, and discard \"STOP\" tag and pick the tag before \"STOP\" \n",
    "            # store that tag into path\n",
    "        else:\n",
    "            currentLayer = piList[len(piList) - (i+1)]\n",
    "            targetTag = path[i-1]\n",
    "            scoreDict = {}\n",
    "            for tags in currentLayer:\n",
    "                # calculate the score of currentLayer[tags]*giantSecondTransitionDict[tags + targetedTag]\n",
    "                # store the value into scoreDict\n",
    "            # find the highest score in scoreDict, and pick that tag\n",
    "            # the second tag in the tag pair will be appended to the path \n",
    "            # e.g O B-negative, B-negative is appended\n",
    "            # if detected that the first tag in the tag pair == \"START\":\n",
    "                # path.append(\"START\")\n",
    "    path.remove(\"START\")\n",
    "    path.remove(\"STOP\")\n",
    "    result = \"\"\n",
    "    for each word in sentence:\n",
    "        # attach the last element of path to first word of sentence ... so on and so forth\n",
    "        # concatenate result with the attached word tag pair\n",
    "    return result\n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
