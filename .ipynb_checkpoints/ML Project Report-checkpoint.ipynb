{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "01.112 ML Project\n",
    "Done by :\n",
    "\n",
    "Yin Ji Sheng(1001670)\n",
    "Hilda Thian(1001776)\n",
    "Teo Yang Rui(1001518)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "## Annotation of raw data sets\n",
    "### Annotation Process\n",
    "Every student of the course helped annotate 500 tweets/Weibo sentences each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "## Implementation of simple Sentiment Analysis using only emission parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading File\n",
    "To easily iterate through a desired file, we strip the white space in the string and put separate words with their tags in a whole list.\n",
    "```\n",
    "def load_original_train():\n",
    "    with open(\"train\", encoding=\"utf-8\") as file:\n",
    "        train_list = file.readlines()\n",
    "        train_list = [x.strip() for x in train_list]\n",
    "        return train_list\n",
    "    \n",
    "train_list = load_original_train()\n",
    "```\n",
    "### Estimating emission parameters\n",
    "+ The method to estimate the emission parameters were as shown in the project description. First, we created an annotation dictionary to store counts of all possible state, {O, I-Positive, I-Negative, I-Neutral, B-Positive, B-Negative, B-Neutral}. Hence, we can call on count(y) anytime when estimating emission parameters without recounting them for every word.\n",
    " \n",
    "+ Then, to estimate emission, we created a function called estimate_emission which take in two variables, word and tag. In this function, the tag will call the annotation dictionary from the first part to yield us the relevant denominator named countBottom.\n",
    " \n",
    "+ For the numerator, a for loop through the loaded train_list above is employed. For each word with their tag, we further split them into a list with 2 elements, [word,tag]. This can only be done if the “word” is not an empty string “”, found between different tweets. Creating an if statement to calculate only when we have a word and tag. We loop through the file and increase numerator count, named countTop when we have the specified word and tag. After this is done, the function returns the fraction countTop/countBottom as the estimated_emission parameter.\n",
    "\n",
    "### Replacing words that appear less than k times with #UNK#\n",
    "+ To replace words that appear less than k times with #UNK#, the function modify_trainingset(k) was created.\n",
    "Firstly, we need to find a way to store the count of words so that we can know the words that have to be replaced. A dictionary called wordCountDict was created. We loop through the entire training set and counting the occurence of each word. \n",
    " \n",
    "+ Secondly, in order to iterate through the file just once and replace the words that needs to be replaced along the way, a list called wordToBeReplacedList was created. If the key in wordCountDict has a value less than k, it will be stored in wordToBeReplacedList.\n",
    " \n",
    "+ Thirdly, we iterate through train_list, and replace the word found in wordToBeReplacedList with #UNK#., then taking into account the original form of the file, the modified set of words and tags are written into a new file called modified_train\n",
    "\n",
    "### Implementing sentiment analysis system to produce tag\n",
    "+ In order to speed up the sentiment analysis algorithm, we created a giantEmissionDict to store all the possible emission parameters into it. This is done using the store_estimate_emission_fix function. The function is also done using the modified training set so we can count the emission of #UNK# as well. \n",
    "\n",
    "+ In sentiment analysis, we first check whether the word exists in the modified training set or not. If it does not, the word is treated as #UNK#. Then, we loop through all the possible tags and get it's emission with respect to that tag and storing them into scoreDict. The tag with the highest emission value is then picked. \n",
    "\n",
    "+ We then use the same sentiment_analysis function for all the words in the entire dev.in file and output the results to dev.p2.out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for Part 2\n",
    "#### CN\n",
    "```\n",
    "#Entity in gold data: 362\n",
    "#Entity in prediction: 3318\n",
    "\n",
    "#Correct Entity : 183\n",
    "Entity  precision: 0.0552\n",
    "Entity  recall: 0.5055\n",
    "Entity  F: 0.0995\n",
    "\n",
    "#Correct Sentiment : 57\n",
    "Sentiment  precision: 0.0172\n",
    "Sentiment  recall: 0.1575\n",
    "Sentiment  F: 0.0310\n",
    "```\n",
    "\n",
    "#### EN\n",
    "```\n",
    "#Entity in gold data: 226\n",
    "#Entity in prediction: 1201\n",
    "\n",
    "#Correct Entity : 165\n",
    "Entity  precision: 0.1374\n",
    "Entity  recall: 0.7301\n",
    "Entity  F: 0.2313\n",
    "\n",
    "#Correct Sentiment : 71\n",
    "Sentiment  precision: 0.0591\n",
    "Sentiment  recall: 0.3142\n",
    "Sentiment  F: 0.0995\n",
    "```\n",
    "\n",
    "#### FR \n",
    "```\n",
    "#Entity in gold data: 223\n",
    "#Entity in prediction: 1149\n",
    "\n",
    "#Correct Entity : 182\n",
    "Entity  precision: 0.1584\n",
    "Entity  recall: 0.8161\n",
    "Entity  F: 0.2653\n",
    "\n",
    "#Correct Sentiment : 68\n",
    "Sentiment  precision: 0.0592\n",
    "Sentiment  recall: 0.3049\n",
    "Sentiment  F: 0.0991\n",
    "```\n",
    "\n",
    "#### SG\n",
    "```\n",
    "#Entity in gold data: 1382\n",
    "#Entity in prediction: 6599\n",
    "\n",
    "#Correct Entity : 794\n",
    "Entity  precision: 0.1203\n",
    "Entity  recall: 0.5745\n",
    "Entity  F: 0.1990\n",
    "\n",
    "#Correct Sentiment : 315\n",
    "Sentiment  precision: 0.0477\n",
    "Sentiment  recall: 0.2279\n",
    "Sentiment  F: 0.0789\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "## Implementation of Viterbi\n",
    "### Estimating transition parameters using Maximum Likelihood Estimation\n",
    "+ We loop through the modified training set and we created a nested list to store tags of each sentence. We also append “START” and “STOP” to the list to allow easy counting later of the transition parameters. \n",
    "\n",
    "+ We then create a dictionary that contains the count of every possible transition in the modified training set. To calculate the transition parameter, we simply divide that count by the count of the start state, which is available to us from annotationDict that we have created earlier. The result is then stored into giantTransitionDict, with key being the transition. \n",
    "\n",
    "### Before implementing Viterbi\n",
    "+ In order to implement the Viterbi algorithm, we need to first split the whole list of words read by the file into a individual tweets. We use the split_into_sentences function to split the whole list of words dev_in_list into a nested list. We first create a list named sentences that is a collection of lists of individual tweets named each_sentence. each_sentence contains words from the same tweet as its elements. The end of each tweet is recognised by an “” empty string. In iterating through the every word in dev_in_list, we check if the word is the empty string “”. If the word is the empty string “”, we are at the end of a tweet so we stop appending words into each_sentence and instead, append each_sentence, now a list of words containing a single completed tweet into sentences. We then have to recreate a new empty list each_sentence to accomodate for the next tweet. If the iterated word is not an empty string “”, the word still belongs to the same tweet and we continue to append it to each_sentence. \n",
    "\n",
    "The following pseudocode will split the data set into sentences:\n",
    "```\n",
    "def split_into_sentences(dev_in_list):\n",
    "    sentences = [] #List to store all tweets, tweet by tweet\n",
    "    each_sentence = [] #List of each tweet, containing words from one tweet\n",
    "    for every word in each line of the file:\n",
    "        if word == \"\": #at end of a tweet\n",
    "            # append each_sentence into list of all tweets\n",
    "            # recreate a new empty list each_sentence to contain words for next tweet\n",
    "        else: #still within the same tweet\n",
    "            # append word into list containing words from this same tweet\n",
    "    return sentences\n",
    "sentences = split_into_sentences(dev_in_list)\n",
    "\n",
    "```\n",
    "\n",
    "### Implementing Viterbi\n",
    "+ There are then two parts to the Viterbi algorithm. Firstly, we have a Viterbi algorithm to recursively compute and store the scores to each tag in every layer of the sequence. The computation equation for the cases are:\n",
    "\n",
    "$$ \\begin{align}\n",
    " & \\\\\n",
    "{\\pi}(k,v)&= max_{u}\\{{\\pi}(k-1, u) * a_{u,v} * b_v(x_k)\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "+ We then find the optimal maximum score and using this optimal score, coupled with the stored scores per layer, we back trace our path to find the optimal parent tags to each layer using the find_backward_path function. The equations involved in back tracing are: \n",
    "\n",
    "$$ \\begin{align}\n",
    " & \\\\\n",
    "{y_{n-1}^*}&= argmax_{u} \\{{\\pi}(n-1,u) * a_{u,y_n^*} \\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "+ Hence, our Viterbi function will run the base case, recursive case and finally, in running the final case, it will incorporate the find_backward_path function. \n",
    "\n",
    "The pseudo codes explaining the algorithms are as follows: \n",
    "```\n",
    "def viterbi(sentence): \n",
    "    beginning = True # to check for start of sentence\n",
    "    piList = [] # list to contain dictionaries of scores of tags of every word\n",
    " \n",
    "    for each word in the tweet:\n",
    "        if word exists in training set:\n",
    "            # no changes are made to the word \n",
    "        else: \n",
    "            # treat word as \"#UNK#\"\n",
    "            \n",
    "        # dictionary to store scores of each tag in a given layer of the sequence\n",
    "        piLayer = {} \n",
    "        if beginning: # base case\n",
    "            for tag in all_tags:\n",
    "                if exists transition probability for 'START' to tag \n",
    "                AND emission probability for tag to word:\n",
    "                    # calculate score of tag and store as pi\n",
    "                    # add pi as a value into the piLayer with the corresponding tag as key\n",
    "            # append piLayer into piList\n",
    "            # update beginning to False\n",
    "\n",
    "        else: # recursive case\n",
    "            previousLayer = piList[len(piList) - 1] # latest layer of scores in piList\n",
    "            if word == \"\": # end of sentence\n",
    "                tempLayer = {} \n",
    "                # final case:\n",
    "                for each previous_tag in the previousLayer:\n",
    "                    if there exists a transition probability for previous_tag to 'STOP': \n",
    "                        # calculate new score and store as pi \n",
    "                        tempLayer[tag] = pi\n",
    "                if tempLayer is not empty:\n",
    "                    lastLayer = {}\n",
    "                    # find the tag that gave the highest score and store it to lastLayer\n",
    "                    # append lastLayer in piList\n",
    "                    # feed piList and sentence into find_path_backward()\n",
    "                    return result from find_path_backward\n",
    "                else:\n",
    "\t\t    # tempLayer should not be empty; signifies an error otherwise\n",
    "                    return \"ERROR\"\n",
    "\n",
    "            else: # if not end of sentence\n",
    "                for tag in all_tags: \n",
    "                    tempLayer = {}\n",
    "                    for each previous_tag in the previousLayer:\n",
    "                        if there exists transition probability for previous_tag to tag \n",
    "                        AND emission probability for tag to word:\n",
    "                            # calculate new score and store as pi\n",
    "                            tempLayer[tag] = pi\n",
    "                    # if empty, it will never transit from any previous_tag to this tag\n",
    "                    if tempLayer is not empty:\n",
    "                        # find the tag that gave the highest score and store it to piLayer\n",
    "                        # using the tag as key and score as its value\n",
    "                # add piLayer into piList\n",
    "\n",
    "```\n",
    "\n",
    "### After running Viterbi\n",
    "The following pseudocode will derive the best path given the piList derived from the Viterbi algorithm:\n",
    "```\n",
    "def find_path_backward(piList, sentence):\n",
    "    path = [] \n",
    "    for i in range(len(piList)): \n",
    "        if i == 0: \n",
    "            # find last layer from piList\n",
    "            # find the tag that produced max value in dictionary lastLayer \n",
    "            # add 'end' as first element to path \n",
    "\n",
    "        elif i == 1: \n",
    "                # add the tag found previously to path \n",
    "\n",
    "        # start of recursive algorithm, back tracing to find optimal parent tag\n",
    "        else: \n",
    "            # find the current layer at position N-i in piList and store as currentLayer \n",
    "            # find the tag (position N-i +1) of interest from path and let it be targetedTag \n",
    "            scoreDict = {}\n",
    "            for each parent tag in currentLayer: \n",
    "                if there exists a transition from parent tag to targetedTag:\n",
    "                    # calculate the score as scoreOfTag\n",
    "                    # add it to scoreDict with corresponding parent tag as key\n",
    "\n",
    "            # find the tag with the highestKey \n",
    "            # add it into path\n",
    "\n",
    "    singleStringToBeWritten = \"\" # new empty string \n",
    "    for every word in the single tweet:\n",
    "        # attach every word to the best tag from the path\n",
    "        # note that the path is derived backwards \n",
    "        # so first word is attached to last tag from path and so on\n",
    "    return singleStringToBeWritten\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for Part 3\n",
    "#### CN\n",
    "```\n",
    "#Entity in gold data: 362\n",
    "#Entity in prediction: 158\n",
    "\n",
    "#Correct Entity : 30\n",
    "Entity  precision: 0.1899\n",
    "Entity  recall: 0.0829\n",
    "Entity  F: 0.1154\n",
    "\n",
    "#Correct Sentiment : 22\n",
    "Sentiment  precision: 0.1392\n",
    "Sentiment  recall: 0.0608\n",
    "Sentiment  F: 0.0846\n",
    "```\n",
    "\n",
    "#### EN\n",
    "```\n",
    "#Entity in gold data: 226\n",
    "#Entity in prediction: 155\n",
    "\n",
    "#Correct Entity : 99\n",
    "Entity  precision: 0.6387\n",
    "Entity  recall: 0.4381\n",
    "Entity  F: 0.5197\n",
    "\n",
    "#Correct Sentiment : 62\n",
    "Sentiment  precision: 0.4000\n",
    "Sentiment  recall: 0.2743\n",
    "Sentiment  F: 0.3255\n",
    "```\n",
    "\n",
    "#### FR \n",
    "```\n",
    "#Entity in gold data: 223\n",
    "#Entity in prediction: 166\n",
    "\n",
    "#Correct Entity : 112\n",
    "Entity  precision: 0.6747\n",
    "Entity  recall: 0.5022\n",
    "Entity  F: 0.5758\n",
    "\n",
    "#Correct Sentiment : 72\n",
    "Sentiment  precision: 0.4337\n",
    "Sentiment  recall: 0.3229\n",
    "Sentiment  F: 0.3702\n",
    "```\n",
    "\n",
    "#### SG\n",
    "```\n",
    "#Entity in gold data: 1382\n",
    "#Entity in prediction: 723\n",
    "\n",
    "#Correct Entity : 135\n",
    "Entity  precision: 0.1867\n",
    "Entity  recall: 0.0977\n",
    "Entity  F: 0.1283\n",
    "\n",
    "#Correct Sentiment : 73\n",
    "Sentiment  precision: 0.1010\n",
    "Sentiment  recall: 0.0528\n",
    "Sentiment  F: 0.0694\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4\n",
    "## Implementation of Max Marginal algorithm\n",
    "\n",
    "### Implementing Max Marginal\n",
    "+ First, we create an empty list called opti_path to store the best sequence of tags as we move along the max_marginal algorithm later on. \n",
    "\n",
    "+ As the max marginal algorithm involves calculating the best expected path from start to state u at the ith sequence, where u is a set comprising of all possible tags, the all_tags list was created so that we can loop through all the tags to determine the best tag with highest Alpha(u,i)*Beta(u,i) for observation i.\n",
    "\n",
    "+ In calculating alpha, we first initialised 2 empty dictionary, forward and alpha_base. Forward is a dictionary initialised so that we can easily extract the alpha values we have calculated by referencing the index of the word in question or index of observation. For word 1, the alpha values will be stored in the forward dictionary as forward={0:{O:alpha value, B-negative alpha value…}, 1:{O:....}...N:{O:...}} . Alpha base is a dictionary use to store the first alpha values, which is just the transition from start to the first tag. Below is a discription of how the code works.\n",
    "\n",
    "The pseudocode below follows the implementation of max marginal algorithm to do decoding:\n",
    "```\n",
    "def max_marginal(sentence):\n",
    "    opti_path=[]\n",
    "    # to store all possible alphas to be called during max marginal\n",
    "    forward={} \n",
    "    alpha_base={}\n",
    "    for u in all_tags:\n",
    "        if start to u transition in giantTransitionDict:\n",
    "            alpha start to u= transition start to u\n",
    "        else:\n",
    "            alpha start to u = 0\n",
    "    forward[0]=alpha_base\n",
    "    \n",
    "    for word in sentence:\n",
    "        # if word is not in ModifiedwordDict, word=#UNK#\n",
    "        # dictionary for values of all tags \n",
    "        tempAlpha={}\n",
    "        for u in all tags:\n",
    "            # initialise new alpha\n",
    "            alpha=0 \n",
    "            # run through all possible transition and emission probabilities to get alpha\n",
    "            for v in all tags: \n",
    "                a=0\n",
    "                b=0\n",
    "                if transition v to u in giantTransitionDict:\n",
    "                    a=transition v to u \n",
    "                # do the same for emission v emit word using giantemissiondict\n",
    "                # call index of word and appropriate tag from forward dictionary\n",
    "                alpha+= forward[index of word][v]*a*b\n",
    "            # store all alpha u in that observation.\n",
    "            tempAlpha[u]=Alpha \n",
    "        # build forward dictionary full of alphas\n",
    "        forward[index of observation]= tempAlpha \n",
    "    \n",
    "    # same for backwards, where base case starts with lastword\n",
    "    lastWord= sentence[len(sentence)-1]:\n",
    "    # if lastword in modifiedWordDict, Lastword=lastword, else lastword=UNK\n",
    "    # calculate beta_base\n",
    "    for i in range(len(sentence),0,-1): \n",
    "        # to move from last word to first word sequentially\n",
    "        # same procedure as forward\n",
    "    \n",
    "    # officially doing max marginal\n",
    "    #for each word: \n",
    "        # iterate through all_tags to calculate alpha(tag)*beta(tag)\n",
    "        # store each calculated value in a dictionary\n",
    "        #append the tag with highest probability to opti_path\n",
    "    \n",
    "    \n",
    "    # attach tag to word\n",
    "    for sentence in sentences:\n",
    "        #call a function that combines the tags and words together\n",
    "        sentence_with_tags = combine_path(opti_path, sentence)\n",
    "    return sentence_with_tags\n",
    "\n",
    "```\n",
    "### After running Max Marginal\n",
    "We create a function called combine_path that will help us attach the best tag to each word later after the max marginal decoding has been run. The following code is the implementation:\n",
    "```\n",
    "def combine_path(best_path, sentence):\n",
    "    result = \"\"\n",
    "    sentence.append(\"\")\n",
    "    for i in range(len(sentence)):\n",
    "        if (sentence[i] == ''):\n",
    "            result += \" \\n\"\n",
    "        else:\n",
    "            result += sentence[i] + \" \" + best_path[i] + \"\\n\"\n",
    "    return result\n",
    "```\n",
    "\n",
    "### Results for Part 4\n",
    "Results for CN and SG are omitted as they are not required as part of the submission.\n",
    "\n",
    "#### EN\n",
    "```\n",
    "#Entity in gold data: 226\n",
    "#Entity in prediction: 172\n",
    "\n",
    "#Correct Entity : 104\n",
    "Entity  precision: 0.6047\n",
    "Entity  recall: 0.4602\n",
    "Entity  F: 0.5226\n",
    "\n",
    "#Correct Sentiment : 69\n",
    "Sentiment  precision: 0.4012\n",
    "Sentiment  recall: 0.3053\n",
    "Sentiment  F: 0.3467\n",
    "```\n",
    "\n",
    "#### FR \n",
    "```\n",
    "#Entity in gold data: 223\n",
    "#Entity in prediction: 173\n",
    "\n",
    "#Correct Entity : 113\n",
    "Entity  precision: 0.6532\n",
    "Entity  recall: 0.5067\n",
    "Entity  F: 0.5707\n",
    "\n",
    "#Correct Sentiment : 73\n",
    "Sentiment  precision: 0.4220\n",
    "Sentiment  recall: 0.3274\n",
    "Sentiment  F: 0.3687\n",
    "```\n",
    "\n",
    "\n",
    "# Part 5\n",
    "## Implementation of Viterbi with second order dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before implementing Second Order Viterbi\n",
    "We had to create a new transition parameter to takes into account the tags two states before. Thus, we had to create a new ```giantSecondTransitionDict```. It stores all the possible transition two states before and its value. Below is the pseudocode for the function that was used to create it:\n",
    "\n",
    "```\n",
    "def store_second_order_transition():\n",
    "    startStateCount = {}\n",
    "    transitionCount = {}\n",
    "    giantSecondTransitionDict = {}\n",
    "    for each_sentence_with_only_tags in entire_data_set_with_only_tags:\n",
    "        for i in range(len(each_sentence_with_only_tags)-1):\n",
    "            # count the occurence of the first two states\n",
    "            # store it to startStateCount, e.g START O\n",
    "        for i in range(len(each_sentence_with_only_tags)-2):\n",
    "            # count the occurence of each transition, e.g, START O O, \n",
    "            # store it to transitionCount\n",
    "    for each_second_order_transition in transitionCount:\n",
    "        # calculate the transition parameter \n",
    "        # divide corresponding values of transitionCount by startStateCount\n",
    "        # store result into giantSecondTransitionDict\n",
    "    return giantSecondTransitionDict    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Second Order Viterbi\n",
    "+ The algorithm follows the following equations to calculate the best scores for each word:\n",
    "\n",
    "$$ \\begin{align}\n",
    " & \\\\\n",
    "{\\pi(k,\\langle u,v \\rangle)}&= max_{u} \\{{\\pi}(k-1,\\langle u,v \\rangle) * a_{t,u,v} * b_v(x_k) \\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The pseudocode below follows the implementation of the second order viterbi using the ```giantSecondTransitionDict``` created earlier:\n",
    "```\n",
    "def second_order_viterbi(sentence):\n",
    "    for each_word in sentence:\n",
    "        if each_word not in training set:\n",
    "            # treat it as #UNK#\n",
    "        \n",
    "        if at first word:\n",
    "            for each tag:\n",
    "                # piLayer = transition from START to tags\n",
    "                # store piLayer into piList\n",
    "        elif at second word:\n",
    "            for each tag:\n",
    "                # piLayer = transition from START and previous_tag to tags\n",
    "                # store piLayer into piList\n",
    "        else:\n",
    "            if word == \"\":\n",
    "                for previous_tags in previousLayer:\n",
    "                    # piLayer = transition from previous_tags to STOP\n",
    "                    # store piLayer into piList \n",
    "                # run backtrack_second_order(piList, sentence)\n",
    "                # in order to recover the path\n",
    "                \n",
    "            else:\n",
    "                for each tag:\n",
    "                    for previous_tags in previousLayer:\n",
    "                        # piLayer = transition from previous_tags to tags\n",
    "                        # store piLayer into piList \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtracking in Second Order Viterbi\n",
    "+ The code also follows the following equation to backtrack and find the optimal path given scores:\n",
    "\n",
    "$$ \\begin{align}\n",
    " & \\\\\n",
    "{y_{n}^*}&= argmax_{u} \\{{\\pi}(n+1,\\langle u,STOP \\rangle) \\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$ \\begin{align}\n",
    " & \\\\\n",
    "{y_{k}^*}&= argmax_{t} \\{{\\pi}(k + 1,\\langle t,y_{k+1}^* \\rangle) * a_{u,y_{k+1}^*, y_{k+2}^*} \\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The following pseudocode allows the discovery of the optimal path by backtracking from the optimal score stored at each word:\n",
    "```\n",
    "def backtrack_second_order(piList, sentence):\n",
    "    path = []\n",
    "    # each key in piList's piLayer:\"O STOP\": 0.001\n",
    "    for i in range(len(piList)-1):\n",
    "        if i == 0:\n",
    "            lastLayer = piList[len(piList) - (i+1)]\n",
    "            # pick the highest value, and discard \"STOP\"\n",
    "            # tag and pick the tag before \"STOP\" \n",
    "            # store that tag into path\n",
    "        else:\n",
    "            currentLayer = piList[len(piList) - (i+1)]\n",
    "            targetTag = path[i-1]\n",
    "            scoreDict = {}\n",
    "            for tags in currentLayer:\n",
    "                # calculate the score \n",
    "                # score = currentLayer[tags]*giantSecondTransitionDict[tags + targetTag]\n",
    "                # store the value into scoreDict\n",
    "            # find the highest score in scoreDict, and pick that tag\n",
    "            # the second tag in the tag pair will be appended to the path \n",
    "            # e.g O B-negative, B-negative is appended\n",
    "            # if detected that the first tag in the tag pair == \"START\":\n",
    "                # path.append(\"START\")\n",
    "    path.remove(\"START\")\n",
    "    path.remove(\"STOP\")\n",
    "    result = \"\"\n",
    "    for each word in sentence:\n",
    "        # attach the last element of path to first word of sentence, and so on\n",
    "        # concatenate result with the attached word tag pair\n",
    "    return result\n",
    "            \n",
    "```\n",
    "### Result for Part 5\n",
    "Results for SG and CN are omitted as they are not required for submission.\n",
    "\n",
    "### EN\n",
    "```\n",
    "#Entity in gold data: 226\n",
    "#Entity in prediction: 141\n",
    "\n",
    "#Correct Entity : 77\n",
    "Entity  precision: 0.5461\n",
    "Entity  recall: 0.3407\n",
    "Entity  F: 0.4196\n",
    "\n",
    "#Correct Sentiment : 48\n",
    "Sentiment  precision: 0.3404\n",
    "Sentiment  recall: 0.2124\n",
    "Sentiment  F: 0.2616\n",
    "```\n",
    "\n",
    "### FR\n",
    "```\n",
    "#Entity in gold data: 223\n",
    "#Entity in prediction: 159\n",
    "\n",
    "#Correct Entity : 35\n",
    "Entity  precision: 0.2201\n",
    "Entity  recall: 0.1570\n",
    "Entity  F: 0.1832\n",
    "\n",
    "#Correct Sentiment : 19\n",
    "Sentiment  precision: 0.1195\n",
    "Sentiment  recall: 0.0852\n",
    "Sentiment  F: 0.0995\n",
    "```\n",
    "\n",
    "### Analysis of Accuracy\n",
    "+ Upon evaluation, it is observed that the score produced by Second-order Viterbi as an alternative is lower than the original Viterbi. This reduced score may be attributed to the fact that second-order viterbi is much stricter in its estimation as it now has to depend on an a sequence of two previous consecutive states instead of one. Using second-order Viterbi, there should be 7^3 possible transitions altogether since there are 7 different sentiment tags. However, there are only 56 transitions available from the training data. Given that the training set provided was very small and limited, the efficiency and accuracy of the second-order viterbi is largely restricted. Assuming that the limitations of the dataset outweighs the supposed increased level of accuracy in the second-order viterbi, we believe that the second-order viterbi is still better than the original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
