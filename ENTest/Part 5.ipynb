{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5\n",
    "### Implementation of a second order HMM and decoding with Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_original_train():\n",
    "    with open(\"train\", encoding=\"utf-8\") as file:\n",
    "        train_list = file.readlines()\n",
    "        train_list = [x.strip() for x in train_list]\n",
    "        listToBeReturned = []\n",
    "        for word_tag_pair in train_list:\n",
    "            \n",
    "            if word_tag_pair != \"\":\n",
    "                word_tag_pair_list = word_tag_pair.split(\" \")\n",
    "                lower_word = word_tag_pair_list[0].lower()\n",
    "                listToBeReturned.append(lower_word + \" \" + word_tag_pair_list[len(word_tag_pair_list)-1])\n",
    "            else:\n",
    "                listToBeReturned.append(\"\")\n",
    "#         print(listToBeReturned)\n",
    "        return listToBeReturned\n",
    "    \n",
    "train_list = load_original_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_files():\n",
    "    with open(\"dev.in\", encoding=\"utf-8\") as file:\n",
    "        dev_in_list = file.readlines()\n",
    "        dev_in_list = [x.strip() for x in dev_in_list]\n",
    "        \n",
    "        return dev_in_list\n",
    "    \n",
    "def load_modified_train_file():\n",
    "    with open(\"modified_train\", encoding=\"utf-8\") as file:\n",
    "        modified_train_list = file.readlines()\n",
    "        modified_train_list = [x.strip() for x in modified_train_list]\n",
    "        listToBeReturned = []\n",
    "        for word_tag_pair in train_list:\n",
    "            \n",
    "            if word_tag_pair != \"\":\n",
    "                word_tag_pair_list = word_tag_pair.split(\" \")\n",
    "                lower_word = word_tag_pair_list[0].lower()\n",
    "                listToBeReturned.append(lower_word + \" \" + word_tag_pair_list[len(word_tag_pair_list)-1])\n",
    "            else:\n",
    "                listToBeReturned.append(\"\")\n",
    "        return listToBeReturned\n",
    "        \n",
    "def split_into_sentences(dev_in_list):\n",
    "    sentences = []\n",
    "    each_sentence = []\n",
    "    for word in dev_in_list:\n",
    "        if word == \"\":\n",
    "            sentences.append(each_sentence)\n",
    "            each_sentence = []\n",
    "        else:\n",
    "            each_sentence.append(word)\n",
    "        \n",
    "    return sentences\n",
    "\n",
    "modified_train_list = load_modified_train_file()   \n",
    "dev_in_list = load_files()\n",
    "sentences = split_into_sentences(dev_in_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 24242, 'I-positive': 607, 'B-positive': 1208, 'I-negative': 133, 'B-negative': 382, 'I-neutral': 23, 'B-neutral': 65}\n"
     ]
    }
   ],
   "source": [
    "annotationDict = {\"O\": 0, \"I-positive\": 0, \"B-positive\": 0, \"I-negative\":0, \"B-negative\": 0, \"I-neutral\":0, \"B-neutral\":0}\n",
    "\n",
    "for i in range(len(train_list)):\n",
    "    annotated_word = train_list[i]\n",
    "    each_word = annotated_word.split(\" \")\n",
    "    if (len(each_word) == 2):\n",
    "        if each_word[1] == \"O\":\n",
    "            annotationDict[\"O\"] += 1\n",
    "        elif each_word[1] == \"I-positive\":\n",
    "            annotationDict[\"I-positive\"] += 1\n",
    "        elif each_word[1] == \"B-positive\":\n",
    "            annotationDict[\"B-positive\"] += 1\n",
    "        elif each_word[1] == \"I-negative\":\n",
    "            annotationDict[\"I-negative\"] += 1\n",
    "        elif each_word[1] == \"B-negative\":\n",
    "            annotationDict[\"B-negative\"] += 1\n",
    "        elif each_word[1] == \"I-neutral\":\n",
    "            annotationDict[\"I-neutral\"] += 1\n",
    "        elif each_word[1] == \"B-neutral\":\n",
    "            annotationDict[\"B-neutral\"] += 1\n",
    "\n",
    "print(annotationDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_word_tag_pair_dict():\n",
    "    word_tag_pair_dict= {}\n",
    "    for word_tag_pair in modified_train_list:\n",
    "        if word_tag_pair_dict.get(word_tag_pair) == None:\n",
    "            word_tag_pair_dict[word_tag_pair] = 1\n",
    "        else:\n",
    "            word_tag_pair_dict[word_tag_pair] += 1\n",
    "    return word_tag_pair_dict\n",
    "\n",
    "def get_word_dict():\n",
    "    modifiedWordDict = {}        \n",
    "    for i in range(len(modified_train_list)):\n",
    "        annotated_word = modified_train_list[i]\n",
    "        each_word = annotated_word.split(\" \")\n",
    "        if each_word[0] in modifiedWordDict:\n",
    "            modifiedWordDict[each_word[0]] += 1\n",
    "        else:\n",
    "            modifiedWordDict[each_word[0]] = 1\n",
    "\n",
    "    return modifiedWordDict\n",
    "    \n",
    "\n",
    "def store_estimate_emission_fix():\n",
    "    giantEmissionDict = {}\n",
    "    for word_tag_pair in modified_train_list:\n",
    "        if word_tag_pair != \"\":\n",
    "            word_tag_pair_list = word_tag_pair.split(\" \")\n",
    "            countTop = wordTagPairDict[word_tag_pair]\n",
    "            countBottom = annotationDict[word_tag_pair_list[len(word_tag_pair_list)-1]]\n",
    "            emission = countTop/countBottom\n",
    "            if giantEmissionDict.get(word_tag_pair) == None:\n",
    "                giantEmissionDict[word_tag_pair] = countTop/countBottom\n",
    "    return giantEmissionDict\n",
    "\n",
    "def store_estimate_transition():\n",
    "    giantTransitionDict = {}\n",
    "    finalGiantTransitionDict = {}\n",
    "    for taggedEachTweet in giantTaggedEachTweet:\n",
    "        for i in range(len(taggedEachTweet) - 1):\n",
    "            if giantTransitionDict.get(taggedEachTweet[i]+\" \"+taggedEachTweet[i+1]) == None:\n",
    "                giantTransitionDict[taggedEachTweet[i]+\" \"+taggedEachTweet[i+1]] = 1\n",
    "            else:\n",
    "                giantTransitionDict[taggedEachTweet[i]+\" \"+taggedEachTweet[i+1]] += 1\n",
    "    for eachTransition in giantTransitionDict:\n",
    "        eachTransitionList = eachTransition.split(\" \")\n",
    "        startState = eachTransitionList[0]\n",
    "        countTop = giantTransitionDict[eachTransition]\n",
    "        countBottom = totalStateNumber[startState]\n",
    "        transitionParam = countTop/countBottom\n",
    "        finalGiantTransitionDict[eachTransition] = transitionParam\n",
    "        \n",
    "    return finalGiantTransitionDict\n",
    "\n",
    "def separate_tweets():\n",
    "    countStart = 1\n",
    "    # counting starts\n",
    "    giant_each_tweet = []\n",
    "    each_tweet = []\n",
    "    totalStateNumber = {}\n",
    "    for i in range(len(modified_train_list)):\n",
    "        annotated_word = modified_train_list[i]\n",
    "        each_word = annotated_word.split(\" \")\n",
    "        if each_word == ['']:\n",
    "            countStart += 1\n",
    "            giant_each_tweet.append(each_tweet)\n",
    "            each_tweet = []\n",
    "\n",
    "        else:\n",
    "            each_tweet.append(each_word[len(each_word)-1])\n",
    "    for i in range(len(giant_each_tweet)):\n",
    "        giant_each_tweet[i].insert(0, 'START')\n",
    "        giant_each_tweet[i].append('STOP')\n",
    "    \n",
    "    for each_tweet in giant_each_tweet:\n",
    "        for tag in each_tweet:\n",
    "            if totalStateNumber.get(tag) == None:\n",
    "                totalStateNumber[tag] = 1\n",
    "            else:\n",
    "                totalStateNumber[tag] += 1\n",
    "\n",
    "    return giant_each_tweet, countStart, totalStateNumber\n",
    "\n",
    "modifiedWordDict = get_word_dict()\n",
    "wordTagPairDict = count_word_tag_pair_dict()\n",
    "giantEmissionDict = store_estimate_emission_fix()\n",
    "giantTaggedEachTweet, countStart, totalStateNumber = separate_tweets()\n",
    "giantTransitionDict = store_estimate_transition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'START O O': 0.7756956274843839, 'O O O': 0.8720445062586927, 'O O STOP': 0.07620737614502901, 'START B-neutral O': 0.9, 'B-neutral O O': 0.9019607843137255, 'START O B-positive': 0.16524701873935263, 'O B-positive O': 0.6900532859680284, 'B-positive O O': 0.7884615384615384, 'O O B-negative': 0.013668409189007722, 'O B-negative O': 0.7817679558011049, 'B-negative O STOP': 0.12374581939799331, 'O O B-positive': 0.0365929691621505, 'B-positive O STOP': 0.15264423076923078, 'START O B-negative': 0.04031800113571834, 'B-negative O O': 0.862876254180602, 'O B-negative I-negative': 0.20994475138121546, 'B-negative I-negative I-negative': 0.3875, 'I-negative I-negative O': 0.5849056603773585, 'I-negative O O': 0.8, 'O O B-neutral': 0.001486739245120138, 'O B-neutral O': 0.7636363636363637, 'O B-positive I-positive': 0.29573712255772644, 'B-positive I-positive O': 0.6083333333333333, 'I-positive O O': 0.7549295774647887, 'START O B-neutral': 0.01306076093128904, 'B-positive I-positive I-positive': 0.38055555555555554, 'I-positive I-positive O': 0.5506072874493927, 'START B-positive I-positive': 0.32926829268292684, 'B-negative O B-negative': 0.013377926421404682, 'START O STOP': 0.0056785917092561046, 'START B-positive O': 0.6707317073170732, 'I-positive I-positive I-positive': 0.44534412955465585, 'I-positive O STOP': 0.18028169014084508, 'I-negative O STOP': 0.175, 'O B-neutral I-neutral': 0.21818181818181817, 'B-neutral I-neutral I-neutral': 0.46153846153846156, 'I-neutral I-neutral O': 0.6, 'I-neutral O O': 0.7692307692307693, 'B-positive O B-positive': 0.05889423076923077, 'I-positive O B-positive': 0.0647887323943662, 'O B-negative STOP': 0.008287292817679558, 'B-negative I-negative O': 0.6125, 'I-neutral I-neutral I-neutral': 0.4, 'I-neutral O STOP': 0.15384615384615385, 'START B-negative I-negative': 0.2, 'B-positive I-positive STOP': 0.011111111111111112, 'I-negative I-negative I-negative': 0.41509433962264153, 'START B-negative O': 0.8, 'B-neutral O STOP': 0.09803921568627451, 'O B-positive STOP': 0.014209591474245116, 'I-positive I-positive STOP': 0.004048582995951417, 'O B-neutral STOP': 0.01818181818181818, 'B-neutral I-neutral O': 0.5384615384615384, 'START B-neutral I-neutral': 0.1, 'I-neutral O B-neutral': 0.07692307692307693, 'I-negative O B-negative': 0.025}\n"
     ]
    }
   ],
   "source": [
    "def store_second_order_transition():\n",
    "    transitionCount = {}\n",
    "    startStateCount = {}\n",
    "    giantSecondTransitionDict = {}        \n",
    "    for taggedEachTweet in giantTaggedEachTweet:\n",
    "        for i in range(len(taggedEachTweet) - 1):\n",
    "            if startStateCount.get(taggedEachTweet[i]+\" \"+taggedEachTweet[i+1]) == None:\n",
    "                startStateCount[taggedEachTweet[i]+\" \"+taggedEachTweet[i+1]] = 1\n",
    "            else:\n",
    "                startStateCount[taggedEachTweet[i]+\" \"+taggedEachTweet[i+1]] += 1\n",
    "                \n",
    "        for i in range(len(taggedEachTweet) - 2):\n",
    "            if transitionCount.get(taggedEachTweet[i]+\" \"+taggedEachTweet[i+1] + \" \" + taggedEachTweet[i+2]) == None:\n",
    "                transitionCount[taggedEachTweet[i]+\" \"+taggedEachTweet[i+1]+ \" \"+ taggedEachTweet[i+2]] = 1\n",
    "            else:\n",
    "                transitionCount[taggedEachTweet[i]+\" \"+taggedEachTweet[i+1]+ \" \"+ taggedEachTweet[i+2]] += 1\n",
    "    for eachTransition in transitionCount:\n",
    "        eachTransitionList = eachTransition.split(\" \") #START O O\n",
    "        startState = eachTransitionList[0] + \" \" + eachTransitionList[1] # START O\n",
    "        countTop = transitionCount[eachTransition]\n",
    "        countBottom = startStateCount[startState]\n",
    "        transitionParam = countTop/countBottom\n",
    "        giantSecondTransitionDict[eachTransition] = transitionParam\n",
    "\n",
    "    return giantSecondTransitionDict\n",
    "\n",
    "giantSecondTransitionDict = store_second_order_transition()\n",
    "print(giantSecondTransitionDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backtrack_second_order(piList, sentence):\n",
    "    print(piList)\n",
    "    print(sentence)\n",
    "    path = []\n",
    "    singleStringToBeWritten = \"\"\n",
    "    all_tags=[\"O\",\"B-negative\",\"B-positive\",\"B-neutral\",\"I-negative\",\"I-positive\",\"I-neutral\"]\n",
    "        \n",
    "    if len(piList) == len(sentence):\n",
    "        for i in range(len(piList)-1):\n",
    "            if i == 0:\n",
    "                lastLayer = piList[len(piList)- (i+1)]\n",
    "                maxPiTag = max(lastLayer, key=lastLayer.get)\n",
    "                maxPiTagList = maxPiTag.split(\" \")\n",
    "                path.append(maxPiTagList[len(maxPiTagList)-1])\n",
    "                path.append(maxPiTagList[0])\n",
    "\n",
    "            else:\n",
    "                currentLayer = piList[len(piList) - (i+1)] #2nd last layer onwards\n",
    "                targetedTag = path[i-1]\n",
    "                scoreDict = {}\n",
    "                for tags in currentLayer:\n",
    "                    if giantSecondTransitionDict.get(tags + \" \" + targetedTag) != None:\n",
    "                        scoreOfTag = currentLayer[tags]*giantSecondTransitionDict[tags + \" \" + targetedTag]\n",
    "                        scoreDict[tags] = scoreOfTag\n",
    "                bestScoreTag = max(scoreDict, key=scoreDict.get)\n",
    "                bestScoreTagList = bestScoreTag.split(\" \")\n",
    "                path.append(bestScoreTagList[0])\n",
    "                if bestScoreTagList[0] == \"START\":\n",
    "                    path.append(bestScoreTagList[len(bestScoreTagList)-1])\n",
    "    path.remove(\"STOP\")\n",
    "    path.remove(\"START\")\n",
    "    for j in range(len(sentence)):\n",
    "        if sentence[j] == \"\":\n",
    "            singleStringToBeWritten += \"\\n\"\n",
    "        else:\n",
    "            singleStringToBeWritten += sentence[j] + \" \" + path[len(path)-(j+1)] + \"\\n\"\n",
    "    return singleStringToBeWritten\n",
    "piList = [{'O': 0.10952242946302841, 'B-negative': 0.0019577571933592874, 'B-positive': 0.010624504726325918, 'B-neutral': 0.0009040105193951349}, {'START O': 0.00024221306515862052, 'START B-negative': 0.0001000958565025801, 'START B-positive': 0.0012160526263888527, 'START B-neutral': 1.9652402595546413e-05}, {'O B-negative': 2.5578783356529717e-08, 'O B-positive': 9.94562791068425e-08}, {'B-positive STOP': 1.413233095656732e-09}]\n",
    "sentence = ['AVOID', 'THAT', 'PLACE', '']\n",
    "piList2 = [{'B-positive': 0.00014504443312390332, 'B-neutral': 8.21827744904668e-05}, {'START B-positive': 1.701560172955939e-06}, {'B-positive O': 7.4384900942866305e-09}, {'O O': 2.2814373537614629e-10}, {'O O': 1.3952466906739993e-13}, {'O O': 2.5096579686371738e-17}, {'O O': 1.375919532845953e-18}, {'O STOP': 1.0482944960703226e-19}]\n",
    "sentence2 = ['lobster', 'was', 'good', ',', 'nothing', 'spectacular', '.', '']\n",
    "# backtrack_second_order(piList, sentence)\n",
    "# backtrack_second_order(piList2, sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'O': 0.0020167704776798356}, {'START O': 3.342790286812497e-05}, {'O O': 7.487383327920167e-09}, {'O O': 5.252122935344911e-11}, {'O O': 3.778636211503018e-15}, {'O O': 1.281790293406509e-16}, {'O O': 2.388454331631745e-18}, {'O O': 1.7183718161449065e-21}, {'O O': 8.035829191372904e-25}, {'O O': 1.4973752835480763e-26}, {'O O': 3.070263993072444e-29}, {'O O': 1.148627638742517e-31}, {'O O': 1.2395690398098006e-35}, {'O O': 2.158173152819642e-37}, {'O O': 4.037010071632344e-40}, {'O O': 3.862884056366419e-42}, {'O O': 1.584115078963055e-44}, {'O O': 1.1396904973940162e-48}, {'O O': 3.866062079752869e-50}, {'O O': 2.837064401826265e-52}, {'O O': 5.286511213664771e-54}, {'O O': 8.13922642592514e-56}, {'O O': 8.783641230001969e-60}, {'O O': 1.5292910743800646e-61}, {'O O': 2.8606432536072368e-64}, {'O O': 2.7372567863927595e-66}, {'O O': 9.285332016557717e-68}, {'O O': 6.680325694107009e-72}, {'O O': 3.2201243176705052e-74}, {'O O': 8.108506742373607e-78, 'O B-positive': 9.754462736325141e-79}, {'O O': 7.875440412651958e-81}, {'O O': 1.1331960310569807e-84}, {'O O': 6.212411505064062e-86}, {'O STOP': 4.734315803341228e-87}]\n",
      "['When', 'I', 'called', 'this', 'morning', ',', 'I', \"didn't\", 'think', 'I', 'would', 'be', 'able', 'to', 'get', 'in', 'at', '12', ',', 'but', 'I', 'was', 'able', 'to', 'get', 'in', ',', 'along', 'with', 'four', 'other', 'guests', '.', '']\n",
      "When O\n",
      "I O\n",
      "called O\n",
      "this O\n",
      "morning O\n",
      ", O\n",
      "I O\n",
      "didn't O\n",
      "think O\n",
      "I O\n",
      "would O\n",
      "be O\n",
      "able O\n",
      "to O\n",
      "get O\n",
      "in O\n",
      "at O\n",
      "12 O\n",
      ", O\n",
      "but O\n",
      "I O\n",
      "was O\n",
      "able O\n",
      "to O\n",
      "get O\n",
      "in O\n",
      ", O\n",
      "along O\n",
      "with O\n",
      "four O\n",
      "other O\n",
      "guests O\n",
      ". O\n",
      "\n",
      "\n",
      "[{'O': 0.05484064337383245, 'B-negative': 5.590605546439763e-05, 'B-positive': 3.6241748372657106e-05, 'B-neutral': 8.213889687461499e-05}, {'START O': 1.5003729839857361e-05, 'START B-positive': 7.86376185206527e-08}, {'O O': 3.7446974480376117e-07, 'B-positive I-positive': 6.825139872407677e-10}, {}, {}, {}, {}, {}, {}]\n",
      "['The', 'tuna', 'and', 'wasabe', 'potatoes', 'are', 'excellent', '.', '']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-79f718b1a270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mgiantStringToBeWritten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0msingleStringToBeWritten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecond_order_viterbi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingleStringToBeWritten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mgiantStringToBeWritten\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msingleStringToBeWritten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-79f718b1a270>\u001b[0m in \u001b[0;36msecond_order_viterbi\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0mpiLayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmaxPiTag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxPiScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mpiList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0msingleStringToBeWritten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbacktrack_second_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;31m#                 print(singleStringToBeWritten)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msingleStringToBeWritten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-ec8ac8ee57e9>\u001b[0m in \u001b[0;36mbacktrack_second_order\u001b[0;34m(piList, sentence)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mlastLayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpiList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mmaxPiTag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlastLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlastLayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mmaxPiTagList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxPiTag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxPiTagList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxPiTagList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "def second_order_viterbi(sentence):\n",
    "    all_tags=[\"O\",\"B-negative\",\"B-positive\",\"B-neutral\",\"I-negative\",\"I-positive\",\"I-neutral\"]\n",
    "    sentence.append(\"\") #so we can detect the end\n",
    "    piList = []\n",
    "    for i in range(len(sentence)):\n",
    "        word = sentence[i].lower()\n",
    "        if word in modifiedWordDict:\n",
    "            word = word\n",
    "        else:\n",
    "            word = \"#UNK#\"\n",
    "            \n",
    "        piLayer = {}\n",
    "        if i == 0:\n",
    "            for tag in all_tags:\n",
    "                if giantTransitionDict.get(\"START\" + \" \" + tag) != None and giantEmissionDict.get(word + \" \" + tag) != None:\n",
    "                    pi = giantTransitionDict[\"START\" + \" \" + tag]*giantEmissionDict[word + \" \" + tag]\n",
    "                    piLayer[tag] = pi\n",
    "            piList.append(piLayer)\n",
    "        \n",
    "        elif i == 1:\n",
    "            previousLayer = piList[len(piList)-1]\n",
    "            for tag in all_tags:\n",
    "                tempLayer = {}\n",
    "                for previous_tag in previousLayer:\n",
    "                    if giantSecondTransitionDict.get(\"START\" + \" \" + previous_tag + \" \" + tag) != None and giantEmissionDict.get(word + \" \" + tag) != None:\n",
    "                        pi = previousLayer[previous_tag]*giantSecondTransitionDict[\"START\" + \" \" + previous_tag + \" \" + tag]*giantEmissionDict[word + \" \" + tag]\n",
    "                        tempLayer[\"START\" + \" \" + previous_tag] = pi\n",
    "                if len(tempLayer) != 0:\n",
    "                    maxPiTag = max(tempLayer, key=tempLayer.get) \n",
    "                    maxPiScore = max(tempLayer.values())\n",
    "                    piLayer[maxPiTag] = maxPiScore\n",
    "            piList.append(piLayer)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            previousLayer = piList[len(piList)-1]\n",
    "            if word == \"\":\n",
    "#                 print(piList)\n",
    "                tempLayer = {}\n",
    "                for previous_tags in previousLayer:\n",
    "                    previous_tags_list = previous_tags.split(\" \")\n",
    "                    if giantSecondTransitionDict.get(previous_tags + \" \" + \"STOP\") != None:\n",
    "                        pi = previousLayer[previous_tags]*giantSecondTransitionDict[previous_tags + \" \" + \"STOP\"]\n",
    "                        tagOneLayerBefore = previous_tags_list[len(previous_tags_list)-1]\n",
    "                        tempLayer[tagOneLayerBefore+ \" \" + \"STOP\"] = pi\n",
    "                if len(tempLayer) != 0:\n",
    "                    maxPiTag = max(tempLayer, key=tempLayer.get) \n",
    "                    maxPiScore = max(tempLayer.values())\n",
    "                    piLayer[maxPiTag] = maxPiScore\n",
    "                piList.append(piLayer)\n",
    "                singleStringToBeWritten = backtrack_second_order(piList, sentence)\n",
    "#                 print(singleStringToBeWritten)\n",
    "                return singleStringToBeWritten\n",
    "            else:\n",
    "                for tag in all_tags:\n",
    "                    tempLayer = {}\n",
    "                    for previous_tags in previousLayer:\n",
    "                        previous_tags_list = previous_tags.split(\" \")\n",
    "                        if giantSecondTransitionDict.get(previous_tags + \" \" + tag) != None and giantEmissionDict.get(word + \" \" + tag) != None:\n",
    "                            pi = previousLayer[previous_tags]*giantSecondTransitionDict[previous_tags + \" \" + tag]*giantEmissionDict[word + \" \" + tag]\n",
    "                            tagOneLayerBefore = previous_tags_list[len(previous_tags_list)-1]\n",
    "                            tempLayer[tagOneLayerBefore+ \" \" + tag] = pi\n",
    "                    if len(tempLayer) != 0:\n",
    "                        maxPiTag = max(tempLayer, key=tempLayer.get) \n",
    "                        maxPiScore = max(tempLayer.values())\n",
    "                        piLayer[maxPiTag] = maxPiScore\n",
    "                piList.append(piLayer)\n",
    "    \n",
    "                        \n",
    "\n",
    "            \n",
    "giantStringToBeWritten = \"\"    \n",
    "for sentence in sentences:\n",
    "    singleStringToBeWritten = second_order_viterbi(sentence)\n",
    "    print(singleStringToBeWritten)\n",
    "    giantStringToBeWritten += singleStringToBeWritten\n",
    "print(giantStringToBeWritten)\n",
    "f = open(\"dev.p5.out\",\"w+\")\n",
    "f.write(giantStringToBeWritten)\n",
    "# second_order_viterbi([\"The\", \"tuna\", \"and\", \"wasabe\", \"potatoes\", \"are\", \"excellent\", \".\"])\n",
    "# second_order_viterbi([\"lobster\", \"was\", \"good\", \",\", \"nothing\",\"spectacular\",\".\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
